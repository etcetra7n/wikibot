{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c0686-787c-4eb0-9d73-2822717c0af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_simple_wiki_dataset():\n",
    "    data = load_dataset(\"rahular/simple-wikipedia\")\n",
    "    return data['train']['text']\n",
    "\n",
    "data = load_simple_wiki_dataset()[:700000]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136979a-d2de-4647-80bd-f5faab88f411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_characters_in_data(data):\n",
    "    characters = set()\n",
    "    for sentence in data:\n",
    "        characters.update(set(sentence.lower()))\n",
    "    return sorted(list(characters))\n",
    "\n",
    "characters = find_characters_in_data(data)\n",
    "characters = [i for i in characters if ord(i)<123]\n",
    "characters.remove('\\\\')\n",
    "characters.remove('@')\n",
    "characters.remove('#')\n",
    "characters.remove(';')\n",
    "characters.remove('`')\n",
    "characters.remove('^')\n",
    "print(characters)\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43cb76-baee-408c-9c3f-37182e2a581e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, characters):\n",
    "        self.characters = characters\n",
    "        self.pad_token = 0\n",
    "        self.bos_token = 1\n",
    "        self.unk_token = 2\n",
    "        self.vocab_size = len(characters)+3\n",
    "    def encode(self, sentence, add_bos_token=False):\n",
    "        encoded = []\n",
    "        if add_bos_token:\n",
    "            encoded.append(self.bos_token)\n",
    "        sentence = sentence.lower()\n",
    "        for char in sentence:\n",
    "            if char not in self.characters:\n",
    "                encoded.append(self.unk_token)\n",
    "            else:\n",
    "                encoded.append(self.characters.index(char)+3)\n",
    "        return torch.LongTensor(encoded)\n",
    "    def decode(self, encoded):\n",
    "        output = \"\"\n",
    "        for i in encoded:\n",
    "            if i<3:\n",
    "                continue\n",
    "            output += self.characters[i-3]\n",
    "        return output\n",
    "\n",
    "tokenizer = CharTokenizer(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0b22-c37c-4514-b9b5-104e847259f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index]\n",
    "        encoded = self.tokenizer.encode(sentence)\n",
    "        return encoded\n",
    "dataset = CharDataset(data, tokenizer)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b49958-9f7d-4570-ae5e-cbed6c603745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CharDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, tokenizer, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_data, val_data, test_data = self.split(data)\n",
    "        self.train_dataset = CharDataset(train_data, tokenizer)\n",
    "        self.val_dataset = CharDataset(val_data, tokenizer)\n",
    "        self.test_dataset = CharDataset(test_data, tokenizer)\n",
    "\n",
    "    def collate_fn(self, samples):\n",
    "        return pad_sequence(samples, batch_first=True, padding_value=self.tokenizer.pad_token)\n",
    "        \n",
    "    def split(self, data):\n",
    "        n_train = int(len(data)*0.8)\n",
    "        n_val = int(len(data)*0.1)\n",
    "        train_data = data[:n_train]\n",
    "        val_data = data[n_train:n_train+n_val]\n",
    "        test_data = data[n_train+n_val:]\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def common_dataloader(self, split):\n",
    "        dataset = getattr(self, f'{split}_dataset')\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=(split=='train'), collate_fn=self.collate_fn)\n",
    "    def train_dataloader(self):\n",
    "        return self.common_dataloader('train')\n",
    "    def val_dataloader(self):\n",
    "        return self.common_dataloader('val')\n",
    "    def test_dataloader(self):\n",
    "        return self.common_dataloader('test')\n",
    "\n",
    "datamodule = CharDataModule(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119087ac-8c5c-4f60-976c-5248e9447136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Generator(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, tokenizer):\n",
    "        super().__init__()\n",
    "        self.emb_layer=nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer=nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.out_layer=nn.Linear(hidden_size, vocab_size)\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "        self.loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token)\n",
    "\n",
    "    def forward(self, encoded, hidden=None):\n",
    "        emb=self.emb_layer(encoded)\n",
    "        rnn_out, hidden = self.rnn_layer(emb, hidden)\n",
    "        out=self.out_layer(rnn_out)\n",
    "        return(out, hidden)\n",
    "\n",
    "    def prepend_bos(self, batch):\n",
    "        bs = batch.shape[0]\n",
    "        bos_tokens = torch.full((bs, 1), self.tokenizer.bos_token, device=batch.device)\n",
    "        output = torch.cat((bos_tokens, batch), dim=1)[:, :-1]\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inp = self.prepend_bos(batch)\n",
    "        out, _ = self(inp)\n",
    "        loss = self.loss_fn(out.transpose(2,1), batch)\n",
    "        self.log('loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inp = self.prepend_bos(batch)\n",
    "        out, _ = self(inp)\n",
    "        loss = self.loss_fn(out.transpose(2,1), batch)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inp = self.prepend_bos(batch)\n",
    "        out, _ = self(inp)\n",
    "        loss = self.loss_fn(out.transpose(2,1), batch)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.0001)\n",
    "    \n",
    "    def generate(self, prompt, n_tokens=256):\n",
    "        encoded_prompt = self.tokenizer.encode(prompt, add_bos_token=True)\n",
    "        out, hidden = self(encoded_prompt)\n",
    "        out = out[-1:]\n",
    "        next_token = torch.distributions.Categorical(out.softmax(-1)).sample()\n",
    "        generated_tokens = [next_token]\n",
    "        for _ in range(n_tokens):\n",
    "            out, hidden = self(next_token, hidden)\n",
    "            next_token = torch.distributions.Categorical(out.softmax(-1)).sample()\n",
    "            generated_tokens.append(next_token)\n",
    "        generated_tokens = torch.cat(generated_tokens, dim=0)\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "        \n",
    "generator = Generator(tokenizer.vocab_size, 128, 512, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb25b5-aef9-4218-8967-ae33c12621f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "#generator.to('cuda')\n",
    "trainer.fit(model=generator, \n",
    "    datamodule=datamodule, \n",
    "    #ckpt_path=\"./lightning_logs/version_9/checkpoints/epoch=2-step=13125.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868af61-4297-4e1c-ac0e-8368818081d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('lightning_logs/version_13/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6e209-71da-48fd-be6b-33c1949660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator.load_from_checkpoint(\n",
    "  \"lightning_logs/version_13/checkpoints/epoch=4-step=21875.ckpt\", \n",
    "  tokenizer=tokenizer,\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  embedding_dim = 128,\n",
    "  hidden_size = 512\n",
    ")\n",
    "generator.to('cpu')\n",
    "\n",
    "prompt = 'Que'\n",
    "output = generator.generate(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b017cc-f117-4ee3-a667-7a0777495805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
